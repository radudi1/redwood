[Cache]

# If this is set to false caching will be completely disabled and you can ignore the rest of this file
Enabled = true

# Number of TLS sessions that will be kept in RAM
# This is used for TLS session resumption in order to speed up TLS setup
TlsSessionCacheSize = 10000

# If http response allows longer caching it will be capped to this many seconds.
MaxTtl = 604800

# If don't have specific caching indications (eg: cache-control) we set expiration time to Age ("last-modified" header) / AgeDivisor
AgeDivisor = 10

# A cached object will be revalidated when it's older than this percent of fresh time
# If "staled-while-revalidate" is present object will be revalidated the "sooner" of the 2 values
# When set to 100 and no "stale-while-revalidate" is present revalidation will actually be disabled
# When set to greater than 100 it will reduce "stale-while-revalidate" window
# Values less than 1 are invalid
FreshPercentRevalidate = 80

# Cached object ttls are capped (by MaxAge and RestrictedMaxAge)
# If cache-control (or other headers) allow longer ttls if we don't update their ttls they will eventually expire when cap is reached
# even if server allowed longer caching. In order to prevent ttls are recomputed on each cache hit and we update the cache ttl to the new value
# In order to prevent excesive load on cache (redis etc.) ExpirePercentUpdate sets a limit on when to update or not a cache ttl
# If new ttl would extend expire time with at least ExpirePercentUpdate % of object age a cache ttl update will occur. Otherwise it will not.
# Set to 100 to never update cache ttls - cached objects might expire before server ttl and they will be refetched - possibly reducing hit ratio
# Set to 0 to always update cache ttl - increased cache (redis etc.) load
ExpirePercentUpdate = 50

# For some mimetypes (eg: video) we can set lower caching ttl.
# RestrictedMimePrefixes is an array of these mimetypes and RestrictedMaxAge is the maximum cache time in seconds
RestrictedMaxAge = 3600
RestrictedMimePrefixes = [ "video/" ]

# Compression levels for supported algorithms
# Choose wisely to balance computational cost on server with bandwidth and storage space savings
BrotliLevel = 1
GZIPLevel = 1
DeflateLevel = 1

# If set to true it will enable a mechanism which tries to detect servers (such as cloudlfare) that have problems with bumping (mitm interception)
# If such a problem is detected the domain will be added to noBump list
# Future connections to that domain will not be decrypted by Redwood and they will not be cached
AutoAddToNoBump = false


[Redis]

# Redis url to connect to - this is where cache will be saved
Url = "redis://redishost:6379"

# Redis DB number
DBNum = 0

# If we can't connect to Redis we will retry to connect every ConnectRetryInterval seconds for a maximum of NumConnectRetries times
# If NumConnectRetries is set to 0 no retry will be performed and caching will be disabled
# A negative value for NumConnectRetries is equivalent with infinite number of retries
NumConnectRetries = -1
ConnectRetryInterval = 1

# If the response expires earlier than this many seconds from now then it will not be cached to Redis
# If set to 0 all responses will be cached (if cacheable) to Redis
MinTtl = 600

# If response body is larger than this many bytes it will not be cached to redis
MaxBodySize = 33554432

# Maximum number of parallel connections to Redis
# New connections to redis are initiated and closed as needed up to this number
# The number should be a power of 2 - if not it will be floored to the closest power of 2
# Currently there's a hard limit of 256 connections - do not go over this or redwood will crash
MaxNumConn = 4

# Redis can be pipelined
# This should decrease Redis load and improve performance especially for very busy servers
# but with the price of increased latency
# Greater values should increase performance and decrease Redis load but it may increase latency (from Redwood to Redis)
# and possibly produce load spikes on Redis
# MaxPipelineLen sets the maximum number of Redis commands that can be pipelined into a single Redis query
# The number should be a power of 2 - if not it will be floored to the closest power of 2
MaxPipelineLen = 256
# PipelineDeadlineUS sets the maximum amount of time in microseconds until the pipeline is waiting to receive more commands
# Set to 0 to disable waiting - decrease latency at the cost of decreased throughput and increased Redis load
PipelineDeadlineUS = 0


[Ram]

# The maximum number of items (http responses and metadata) to be kept in RAM cache
# If set to 0 RAM caching is disabled
# Beware that responses with Vary header actually have 2 records in cache.
# Beware that fake certificates are also cached.
# Make sure you take all this into account when setting this value
NumItems = 0

# If the response expires earlier than this many seconds from now then it will not be cached to Ram
# If set to 0 all responses will be cached (if cacheable) to Ram
# WARNING!!! When you set this value lower than Redis MinTtl you may get into a situation
# where a cached response may be evicted from RAM and older response still exists in Redis.
# The older response will be served to client!!!
# Choose your values wisely!
MinTtl = 0

# The maximum response body size in bytes to be cached in RAM
# Choosing too big a value can increase RAM consumption to unacceptable values
# Choosing too small a value will make more responses not to be cached in RAM even if they are very hot/popular
MaxBodySize = 32768


[Log]

# ResponseCache has it's own log
LogFile = "/var/log/redwood/responseCache.log"

# Revalidation mechanism (if enabled) has it's own log file
RevalidateLogFile = ""

# Logging messages are buffered. If you set a buffer that is too small processing threads will block waiting for the logging threads to finish
LogBufferSize = 4096


[Workers]

# Writing threads are priorised lower than client threads (http requests)
# This should slightly improve response time to client because cache writing is done in background only when no more clients are waiting for results
# But if you have a very busy server with no idle times (there's always at least one client waiting for response)
# then priorities don't make sends. In this scenario it will eventually behave like no priorities at all but with increased delay and overhead
# Therefore this setting disables priorities and all workers (http clients or cache writers) are given equal opportunity
PrioritiesEnabled = false

# When priorities are enabled lower priority tasks (eg: set, update ttl) will wait until all higher priority tasks finish (eg: get)
# For a busy server this could delay lower priority takss indefinitely
# This setting sets a limit for how many low priority tasks may wait
# When a this limit is reached a low priority task (random) will be allowed to execute regardless of whether higher priority tasks are still running
# Low priority tasks will still be delayed by they will eventually be executed even when high priority takss are running all the time
# Set lower values for lower delays and higher values if you know that there are idle times in client requests coming in and delays are acceptable
# Consider disabling priorities altogether if you have a very busy server
MaxBlockedWorkers = 1024


# All settings in StandardViolations violate http standards for improved cacheability
# !!! Do NOT use them unless you know what you're doing !!!
# !!! Things could break horribly !!!
# None of the settings will be applied if EnableStandardViolations is set to false
[StandardViolations]

# This is a kill-switch for all settings in StandardViolations section
# If disabled none of the other settings will be in effect (regardless of their value)
# If enabled it will already enable some minor standard violations which should be more or less safe
EnableStandardViolations = false

# If ServeStale is enabled stale responses will be served from cache and revalidation will kick in (in background)
# Make sure that you set RevalidateNumWorkers to something greater than 0
ServerStale = false

# According to standard s-maxage specifically sets intermediate cache ttls (redwood)
# But often times this is set too low compared to maxage
# If OverrideSMaxAge is set to true than cache ttl will be set to the maximum of s-maxage and maxage
OverrideSMaxAge = false

# If set to true then "Expires" header is ignored and cache ttl will be computed from last-modified / AgeDivisor
OverrideExpire = false

# Cache-control maxage headers can be ignored if OverrideCacheControl is set to true
# In this situation the algorithm works like this:
# We compute time since last-modified divided by AgeDivisor and we cap it to OverrideCacheControlMaxAge
# We get max-age from cache-control
# We set the ttl to be the maximum of the 2 values
# If no cache-control and no last-modified headers are present we try to set ttl to Age / AgeDivisor
# If set to true it's behavior implies OverrideExpire set to true as well
OverrideCacheControl = false
OverrideCacheControlMaxAge = 43200

# Is DefaultAge is greater than 0 and there are no headers on which we can compute caching ttl
# the response will be cached for DefaultAge seconds
DefaultAge = 0

# WARNING!!! THIS SETTING ENABLES REALLY DANGEROUS HEURISTICS THAT CAN BREAK THINGS BADLY
# THIS WILL ALSO LEAD TO SERIOUS SECURITY AND PRIVACY PROBLEMS
# DO NOT ENABLE THIS!!!
EnableDangerousHeuristics = false